{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"My name is Kritik Mathur is Kritik\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = \"Kritik\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "ANdar\n",
      "54\n",
      "Accuracy :  47.572815534\n",
      "ANdar\n",
      "65\n",
      "Accuracy :  36.8932038835\n",
      "ANdar\n",
      "53\n",
      "Accuracy :  48.5436893204\n",
      "ANdar\n",
      "65\n",
      "Accuracy :  36.8932038835\n",
      "ANdar\n",
      "60\n",
      "Accuracy :  41.7475728155\n",
      "ANdar\n",
      "69\n",
      "Accuracy :  33.0097087379\n",
      "ANdar\n",
      "61\n",
      "Accuracy :  40.7766990291\n",
      "ANdar\n",
      "61\n",
      "Accuracy :  40.7766990291\n",
      "ANdar\n",
      "62\n",
      "Accuracy :  39.8058252427\n",
      "ANdar\n",
      "59\n",
      "Accuracy :  42.1568627451\n",
      "Maximum accuracy attained  48.5436893204\n",
      "Average accuracy attained  40.8176280221\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from sklearn import naive_bayes as nb\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier as mlpc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from numpy import mean\n",
    "\n",
    "# suffixes = {\n",
    "#     1: [\"ो\", \"े\", \"ू\", \"ु\", \"ी\", \"ि\", \"ा\"],\n",
    "#     2: [\"कर\", \"ाओ\", \"िए\", \"ाई\", \"ाए\", \"ने\", \"नी\", \"ना\", \"ते\", \"ीं\", \"ती\", \"ता\", \"ाँ\", \"ां\", \"ों\", \"ें\"],\n",
    "#     3: [\"ाकर\", \"ाइए\", \"ाईं\", \"ाया\", \"ेगी\", \"ेगा\", \"ोगी\", \"ोगे\", \"ाने\", \"ाना\", \"ाते\", \"ाती\", \"ाता\", \"तीं\", \"ाओं\", \"ाएं\", \"ुओं\", \"ुएं\", \"ुआं\"],\n",
    "#     4: [\"ाएगी\", \"ाएगा\", \"ाओगी\", \"ाओगे\", \"एंगी\", \"ेंगी\", \"एंगे\", \"ेंगे\", \"ूंगी\", \"ूंगा\", \"ातीं\", \"नाओं\", \"नाएं\", \"ताओं\", \"ताएं\", \"ियाँ\", \"ियों\", \"ियां\"],\n",
    "#     5: [\"ाएंगी\", \"ाएंगे\", \"ाऊंगी\", \"ाऊंगा\", \"ाइयाँ\", \"ाइयों\", \"ाइयां\"],\n",
    "# }\n",
    "#\n",
    "# def stem(word):\n",
    "#     for L in 5, 4, 3, 2, 1:\n",
    "#         if len(word) > L + 1:\n",
    "#             for suf in suffixes[L]:\n",
    "#                 if word.endswith(suf):\n",
    "#                     return word[:-L]\n",
    "#     return word\n",
    "\n",
    "\n",
    "stopwords = []\n",
    "project_dir = \"/home/rkb/Documents/IASNLP/\"\t#path where the project files are\n",
    "stopwords_txt = project_dir + \"stopwords\"\n",
    "\n",
    "# data_dir = \"cleaned/\"\n",
    "# classes_dir = [\"pos/\", \"neg/\"]\n",
    "# classes = [\"pos\", \"neg\"]\n",
    "data_dir = \"four_class_data/\"\t\t\t\t#path where the dataset is\n",
    "classes_dir = [\"E/\", \"A/\", \"C/\", \"D/\"]\n",
    "classes = [\"E\", \"A\", \"C\", \"D\"]\n",
    "\n",
    "target = []\n",
    "songs = []\n",
    "\n",
    "def getCorpus():\n",
    "    with open(stopwords_txt) as stops:\n",
    "        for stop in stops:\n",
    "            stopwords.append(stop.strip())\n",
    "\n",
    "    for dir in classes_dir:\n",
    "        folder = project_dir + data_dir + dir\n",
    "        for file in os.listdir(folder):\n",
    "            with open(folder + file) as f:\n",
    "                song = f.read().strip().split()\n",
    "                song = [word for word in song if word not in stopwords]\n",
    "                songs.append(song)\n",
    "                target.append(classes[classes_dir.index(dir)])\n",
    "\n",
    "getCorpus()\n",
    "frequency = defaultdict(int)\n",
    "for song in songs:\n",
    "    for word in song:\n",
    "        frequency[word] += 1\n",
    "\n",
    "songs = [[word for word in song if frequency[word] > 1] for song in songs]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(songs)\n",
    "n_unique_tokens = len(dictionary)\n",
    "\n",
    "bag_of_words = [dictionary.doc2bow(song) for song in songs]\n",
    "\n",
    "#saving for persistency, can be avoided\n",
    "gensim.corpora.MmCorpus.serialize('bag_of_words.mm', bag_of_words)\n",
    "dictionary.save('dictionary.dict')\n",
    "\n",
    "tfidf = gensim.models.TfidfModel(bag_of_words)\n",
    "records = tfidf[bag_of_words]\n",
    "dense_tfidf = gensim.matutils.corpus2dense(records, num_terms = n_unique_tokens).transpose()\n",
    "dense_bow = gensim.matutils.corpus2dense(bag_of_words, num_terms = n_unique_tokens).transpose\n",
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for it in range(1):\n",
    "    print (\"Iteration \", it)\n",
    "    for train, test in kf.split(dense_tfidf):\n",
    "        train_set = []\n",
    "        train_labels = []\n",
    "        test_set = []\n",
    "        test_labels = []\n",
    "        for i in train:\n",
    "            train_set.append(dense_tfidf[i])\n",
    "            train_labels.append(target[i])\n",
    "        for i in test:\n",
    "            test_set.append(dense_tfidf[i])\n",
    "            test_labels.append(target[i])\n",
    "        print (\"ANdar\")\n",
    "\n",
    "        # gnb_tfidf = nb.GaussianNB()\n",
    "        # predicted = gnb_tfidf.fit(train_set, train_labels).predict(test_set)\n",
    "\n",
    "        # gnb_bow = nb.GaussianNB()\n",
    "        # predicted = gnb_bow.fit(train_set, train_labels).predict(test_set)\n",
    "\n",
    "        # mnb_tfidf = svm.SVC()\n",
    "        # predicted = mnb_tfidf.fit(train_set, train_labels).predict(test_set)\n",
    "\n",
    "        knn_tfidf = KNeighborsClassifier()\n",
    "        predicted = knn_tfidf.fit(train_set, train_labels).predict(test_set)\n",
    "\n",
    "#         mlp_tfidf = mlpc(solver = 'lbfgs', hidden_layer_sizes = (15, 5), max_iter = 500)\n",
    "#         predicted = mlp_tfidf.fit(train_set, train_labels).predict(test_set)\n",
    "\n",
    "        #\n",
    "        # for i in range (len(predicted)):\n",
    "        #     print (predicted[i], \" \", test_labels[i])\n",
    "        # print (\"****************************\")\n",
    "\n",
    "        # bern_tfidf = nb.\n",
    "\n",
    "        print ((test_labels != predicted).sum())\n",
    "        incorrect = (test_labels != predicted).sum()\n",
    "        \n",
    "        # incorrects.append(incorrect)\n",
    "        accuracy = (len(test_set) - incorrect) / len(test_set) * 100.\n",
    "        accuracies.append(accuracy)\n",
    "        # print (\"Number of mislabeled points out of a total %d points : %d\" %(len(test_set), incorrect))\n",
    "        print (\"Accuracy : \", accuracy)\n",
    "\n",
    "print (\"Maximum accuracy attained \", max(accuracies))\n",
    "print (\"Average accuracy attained \", mean(accuracies))\n",
    "# print (\"Average accuracy %d\" %((float(len(test_set)) - mean(incorrects)) / len(test_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = \"My name is is kritik mathur is kritik\".split()\n",
    "k = \"kritik\"\n",
    "l = a.index(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(a):\n",
    "    if j == k:\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6 / 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
